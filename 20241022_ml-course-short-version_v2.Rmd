---
title: "ML course Project"
author: "nm"
date: "2024-10-22"
output: html_document
---

## Packages and data
```{r}
# Load required packages
library(tidyverse)
library(caret)
library(randomForest)
library(reshape2)
library(ggplot2)
library(rmarkdown)

#setwd
setwd("C:/Users/NickMoore/OneDrive - Integrity/Documents/ml-course/20241022_ml-course-short-version_v2_files")

# Set seed for reproducibility
set.seed(2332)

#training data
file_path_train <- "C:/Users/NickMoore/OneDrive - Integrity/Documents/ml-course/pml-training.csv"
train_data <- read.csv(file_path_train)

#testing data
file_path_test <- "C:/Users/NickMoore/OneDrive - Integrity/Documents/ml-course/pml-testing.csv"
test_data <- read.csv(file_path_test)
```

## 2. Data Cleaning and key descriptives 
Columns with near-zero variance and high missing values are removed to reduce noise and irrelevant data. Descriptive analysis (bar plot of classe, boxplots of key variables, and a correlation heatmap) help understand the distribution and relationships between predictors, which gave some insight into feature importance and potential multicollinearity. Although I found this exploratory work quite hard to start and very time consuming. 
```{r}
# Remove columns with near-zero variance
nzv_cols <- nearZeroVar(train_data)
train_data <- train_data %>% select(-all_of(nzv_cols))
test_data <- test_data %>% select(-all_of(nzv_cols))

# Remove columns with too many missing values
threshold_na <- 0.95  # Threshold for missing values
na_cols <- train_data %>%
  summarise(across(everything(), ~ mean(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "na_ratio") %>%
  filter(na_ratio > threshold_na) %>%
  pull(feature)

train_data <- train_data %>% select(-all_of(na_cols))
test_data <- test_data %>% select(-all_of(na_cols))

# Remove irrelevant columns
train_data <- train_data %>%
  select(-"X",-"user_name",-"raw_timestamp_part_1",-"raw_timestamp_part_2",-"cvtd_timestamp",-"num_window")
test_data <- test_data %>%
  select(-"X",-"user_name",-"raw_timestamp_part_1",-"raw_timestamp_part_2",-"cvtd_timestamp",-"num_window")
glimpse(train_data)

# Make   'classe' is a factor var
train_data$classe <- as.factor(train_data$classe)

##descriptives

# PLto classe distr.
train_data %>%
  ggplot(aes(x = classe, fill = classe)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Classe in Training Data",
       x = "Classe",
       y = "Count") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "none")

key_vars <- c("roll_belt", "pitch_belt", "yaw_belt", "classe")

# Create a long-format data frame for plotting
train_long <- train_data %>%
  select(all_of(key_vars)) %>%
  pivot_longer(cols = -classe, names_to = "variable", values_to = "value")

# Plot boxplots
train_long %>%
  ggplot(aes(x = classe, y = value, fill = classe)) +
  geom_boxplot() +
  facet_wrap(~ variable, scales = "free_y") +
  theme_minimal() +
  labs(title = "Boxplots of Key Sensor Measurements by Classe",
       x = "Classe",
       y = "Value") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "none")


# Correlation matrix of numeric predictor variables
numeric_vars <- train_data %>%
  select(where(is.numeric), -classe)

cor_matrix <- cor(numeric_vars, use = "pairwise.complete.obs")

# Melt the correlation matrix for plotting
melted_cor <- melt(cor_matrix)

# Plot the heatmap
ggplot(data = melted_cor, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "green", high = "purple", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name="Correlation") +
  theme_minimal() +
  labs(title = "Correlation Heatmap of Predictor Variables") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```
## 3. Partition Training Data
The training data is split into two subsets: 70% for training and 30% for testin; the model is trained on one portion and evaluated on another.
```{r}
train_index <- createDataPartition(train_data$classe, p = 0.7, list = FALSE)
training <- train_data[train_index, ]
testing <- train_data[-train_index, ]
```
## 4. Train Random Forest Model
A Random Forest model is trained using 5-fold cross-validation to minimise overfitting. The model is trained on 100 trees (ntree = 100) using the train function
```{r}
control <- trainControl(method = "cv", number = 5) # Cross-validation with 5 folds (trained on 4 parts/tested on remaining)
model_rf <- train(classe ~ ., data = training, method = "rf", trControl = control, ntree = 100)
model_rf
```
## 5. Evaluate Model on Testing Set
The model is used to make predictions on the testing data. My view is that performs pretty well against key measures, especially accuracy and kappa.
```{r}
predictions <- predict(model_rf, newdata = testing)
confusion <- confusionMatrix(predictions, testing$classe)
print(confusion)

#good accuracy and Kappa, sensitivity and specificity

importance <- varImp(model_rf)
plot(importance, top = 20)

```
## 6. Calculate Out-of-Sample Error
A low error rate, such as 0.61%, suggests the model is highly accurate.But I guess error rate is context dependent. Whats fine here might not be ok elsewhere. Exercise performance might have a bearing on health outcomes so could be something to look at?
```{r}
accuracy <- confusion$overall['Accuracy']
out_of_sample_error <- 1 - accuracy
print(paste("Out-of-sample error:", round(out_of_sample_error, 4)))
# this is good for this type of application but note the links to health/exercise may mean it isnt - its not my field
```
## 7. Predict on Final Test Set
The model is applied to the test set.I got stuck fora while because I forgot to remove classe from the test_data.
```{r}
#remember that classe is removed heree!
final_predictions <- predict(model_rf, newdata = test_data) # n=20 in test_data
```
## 8. Output Predictions
Final predictions for evaluation 
```{r}
print(final_predictions)